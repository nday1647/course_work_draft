{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:27:42.044345",
     "start_time": "2017-04-20T22:27:40.387956"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import xgboost as xgb\n",
    "import pickle as pkl\n",
    "import importlib\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ETC\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "from xgboost.sklearn import XGBClassifier as XGBC\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import scripts.xgboost_tuning as xgb_tuning\n",
    "from sklearn.ensemble import BaggingClassifier as BC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import KernelDensity as KD\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 5\n",
    "import pyeeg\n",
    "import pyrem.univariate as pruni\n",
    "import scipy.signal as sig\n",
    "import scipy.fftpack as fft\n",
    "\n",
    "\n",
    "def recurrence_plot(series, epsilon=0.001):\n",
    "    rp = (np.abs(series - series[:, np.newaxis]) < epsilon).astype(int)\n",
    "    plt.imshow(rp, origin='lower', interpolation='none', cmap='Greys')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def poincare_plot(series):\n",
    "    plt.scatter(series[:-1], series[1:])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def normalized_crosscorrelation(series1, series2):\n",
    "    if series1.shape[0] != series2.shape[0]:\n",
    "        raise BaseException('shapes must be the same')\n",
    "    if (series1.shape[0] == 1) or (series2.shape[0] == 1):\n",
    "        if np.isclose(series1[0], 0.0, rtol=0.0, atol=0.000001) or np.isclose(series2[0], 0.0, rtol=0.0,\n",
    "                                                                              atol=0.000001):\n",
    "            return 0.0\n",
    "        if np.isclose(series1[0], series2[0], rtol=0.0, atol=0.000001):\n",
    "            return 1.0\n",
    "        if np.isclose(abs(series1[0]), abs(series2[0]), rtol=0.0, atol=0.000001):\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "    return ((series1 - series1.mean()) * (series2 - series2.mean())).sum() /\\\n",
    "            (series1.shape[0] * series1.std() * series2.std())\n",
    "\n",
    "\n",
    "def normalized_autocorrelation(series, max_lag=76):\n",
    "    result = []\n",
    "    result.append(normalized_crosscorrelation(series, series))\n",
    "    if max_lag <= 0:\n",
    "        return np.array(result)\n",
    "    if max_lag >= series.shape[0]:\n",
    "        raise BaseException('max_lag must be not more than series length')\n",
    "    temp_series = np.hstack((series, series))\n",
    "    for k in range(1, max_lag + 1):\n",
    "        result.append(normalized_crosscorrelation(temp_series[:-k], temp_series[k:]))\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def recurrence(series, epsilon=0.001):\n",
    "    return (np.abs(series - series[:, np.newaxis]) < epsilon).sum() / (series.shape[0] * series.shape[0])\n",
    "\n",
    "\n",
    "def mean_autocorrelation(series, max_lag=76):\n",
    "    return normalized_autocorrelation(series, max_lag).mean()\n",
    "\n",
    "\n",
    "def mean_period(series, max_lag=76, threshold=-1.0):\n",
    "    '''left local minimum and all local maximums must be uniquely defined'''\n",
    "    if series.shape[0] < 3:\n",
    "        raise BaseException('time series must be longer')\n",
    "    auto_corr = normalized_autocorrelation(series, max_lag)\n",
    "    auto_corr[auto_corr < threshold] = threshold\n",
    "    diffs = np.diff(auto_corr)\n",
    "    prod_diffs = diffs[:-1] * diffs[1:]\n",
    "    susp_extr_dots = np.where(prod_diffs < 0)[0]\n",
    "    start_index = np.array([0])\n",
    "    if (diffs[susp_extr_dots[0]] < 0) and (diffs[susp_extr_dots[0] + 1] > 0):\n",
    "        if susp_extr_dots.shape[0] > 1:\n",
    "            start_index = np.hstack((0, susp_extr_dots[1::2], series.shape[0]))\n",
    "    else:\n",
    "        start_index = np.hstack((0, susp_extr_dots[::2], series.shape[0]))\n",
    "    if start_index.shape[0] == 1:\n",
    "        start_index = np.hstack((start_index, series.shape[0]))\n",
    "    return np.diff(start_index).mean()\n",
    "\n",
    "\n",
    "def poincare_SD(series):\n",
    "    pp = np.hstack((series[:-1][:, np.newaxis], series[1:][:, np.newaxis]))\n",
    "    p1 = np.array([[-1.0 / np.sqrt(2.0)], [1.0 / np.sqrt(2.0)]])\n",
    "    p2 = np.array([[1.0 / np.sqrt(2.0)], [1.0 / np.sqrt(2.0)]])\n",
    "    return np.dot(pp, p1).ravel().std(), np.dot(pp, p2).ravel().std()\n",
    "\n",
    "\n",
    "def DET_and_mean_diag_length(series, epsilon=0.001, min_l=15):\n",
    "    if (min_l > series.shape[0]) or (min_l < 1):\n",
    "        raise BaseException('min_l must be in correct range')\n",
    "    rp = (np.abs(series - series[:, np.newaxis]) < epsilon).astype(np.int)\n",
    "    lines_hist = np.zeros(series.shape[0]).astype(np.int)\n",
    "    isline = False\n",
    "    length = 0\n",
    "    for j in range(1, series.shape[0]):\n",
    "        for k in range(series.shape[0] - j):\n",
    "            if rp[k][j + k]:\n",
    "                if isline:\n",
    "                    length += 1\n",
    "                else:\n",
    "                    isline = True\n",
    "                    length = 1\n",
    "            else:\n",
    "                isline = False\n",
    "                if length:\n",
    "                    lines_hist[length - 1] += 1\n",
    "                    length = 0\n",
    "        isline = False\n",
    "        if length:\n",
    "            lines_hist[length - 1] += 1\n",
    "            length = 0\n",
    "    lines_hist *= 2\n",
    "    lines_hist[-1] += 1\n",
    "    line_lengths = np.arange(series.shape[0]) + 1\n",
    "    mask = line_lengths >= min_l\n",
    "    line_lengths[~mask] = 0\n",
    "    sum_length = (line_lengths * lines_hist).sum()\n",
    "    return sum_length / rp.sum(), sum_length / lines_hist[mask].sum()\n",
    "\n",
    "\n",
    "def FilterBank(X, filters='BandpassBank', sample_rate=128, butter_pows=None):\n",
    "    freq_pairs = None\n",
    "    X_processed = None\n",
    "    if filters == 'BandpassBank':\n",
    "        freq_pairs =  [[0.5], [0.5, 4.0], [4.0, 8.0], [8.0, 13.0], [13.0, 30.0], [30.0, 42.0]]\n",
    "        butter_pows = [5, 6, 8, 11, 11, 10]\n",
    "    else:\n",
    "        freq_pairs = filters\n",
    "    for i in range(len(freq_pairs)):\n",
    "        power = 5 if butter_pows is None else butter_pows[i]\n",
    "        if len(freq_pairs[i]) == 1:\n",
    "            b, a = sig.butter(power, 2 * freq_pairs[i][0] / sample_rate, btype='lowpass')\n",
    "        else:\n",
    "            b, a = sig.butter(power, 2 * np.array(freq_pairs[i]) / sample_rate, btype='bandpass')\n",
    "        X_filtered = sig.lfilter(b, a, X, axis=0)\n",
    "        X_processed = X_filtered if X_processed is None else np.c_[X_processed, X_filtered]\n",
    "    return X_processed\n",
    "\n",
    "\n",
    "def WindowDataset(dataset, interval=77, shift=1):\n",
    "    if (interval < 1) or (interval > dataset.shape[0]):\n",
    "        raise BaseException('interval has invalid value')\n",
    "    if (shift < 1) or (shift > interval):\n",
    "        raise BaseException('shift has invalid value')\n",
    "    windowed_dataset = np.zeros((np.ceil((dataset.shape[0] - interval + 1) / shift).astype(np.int),\n",
    "                                 interval, dataset.shape[1]))\n",
    "    begin = 0\n",
    "    for i in range(windowed_dataset.shape[0]):\n",
    "        windowed_dataset[i] = dataset[begin: begin + interval, :].copy()\n",
    "        begin += shift\n",
    "    return windowed_dataset\n",
    "\n",
    "\n",
    "def StackedRecurrence(X, epsilon=0.001):\n",
    "    return np.sum(np.abs(X - X[:, np.newaxis]) < epsilon, axis=(0, 1)) / (X.shape[0] * X.shape[0])\n",
    "\n",
    "\n",
    "def StackedDET(X, epsilon=0.001, min_l=15):\n",
    "    result = np.zeros(2 * X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[2 * i], result[2 * i + 1] = DET_and_mean_diag_length(series=X[:, i].ravel(),\n",
    "                                                                    epsilon=epsilon, min_l=min_l)\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedPoincareSD(X):\n",
    "    result = np.zeros(2 * X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[2 * i], result[2 * i + 1] = poincare_SD(series=X[:, i].ravel())\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedMeanAutocorrelation(X, max_lag=76):\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = mean_autocorrelation(series=X[:, i].ravel(), max_lag=max_lag)\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedMeanPeriod(X, max_lag=76, threshold=-1.0):\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = mean_period(series=X[:, i].ravel(), max_lag=max_lag, threshold=threshold)\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedAproximateEntropy(X, M=None, R=None):\n",
    "    if M is None:\n",
    "        M = 14 + np.ones(X.shape[1]).astype(np.int)\n",
    "    if R is None:\n",
    "        R = 0.001 + np.zeros(X.shape[1])\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = pyeeg.ap_entropy(X=X[:, i].ravel(), M=M[i], R=R[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedSampleEntropy(X, M=None, R=None):\n",
    "    if M is None:\n",
    "        M = 14 + np.ones(X.shape[1]).astype(np.int)\n",
    "    if R is None:\n",
    "        R = 0.001 + np.zeros(X.shape[1])\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = pyeeg.samp_entropy(X=X[:, i].ravel(), M=M[i], R=R[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedSpectralEntropy(X, sampling_freq=128):\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = pruni.spectral_entropy(a=X[:, i].ravel(), sampling_freq=sampling_freq)\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedPFD(X):\n",
    "    result = np.zeros(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[i] = pruni.pfd(a=X[:, i].ravel())\n",
    "    return result\n",
    "\n",
    "\n",
    "def StackedHjorth(X):\n",
    "    result = np.zeros(3 * X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        result[3 * i], result[3 * i + 1], result[3 * i + 2] = pruni.hjorth(X[:, i].ravel())\n",
    "    return result\n",
    "\n",
    "\n",
    "def TransformWindowedDataset(X_windowed, Y_windowed, parameters=None):\n",
    "    if parameters is None:\n",
    "        parameters = {'FilterBank': {'filters': [[0.5], [0.5, 4.0], [4.0, 8.0], [8.0, 13.0], [13.0, 30.0],\n",
    "                                                 [30.0, 42.0]],\n",
    "                                     'sample_rate': 128, 'butter_pows': [5, 6, 8, 11, 11, 10]},\n",
    "                      'StackedRecurrence': {'epsilon': 0.001},\n",
    "                      'StackedDET': {'epsilon': 0.001, 'min_l': 15},\n",
    "                      'StackedPoincareSD': True,\n",
    "                      'StackedMeanAutocorrelation': {'max_lag': 76},\n",
    "                      'StackedMeanPeriod': {'max_lag': 76, 'threshold': -1.0},\n",
    "                      'StackedAproximateEntropy': {'M': None, 'R': None},\n",
    "                      'StackedSampleEntropy': {'M': None, 'R': None},\n",
    "                      'StackedSpectralEntropy': {'sampling_freq': 128},\n",
    "                      'StackedPFD': True,\n",
    "                      'StackedHjorth': True,\n",
    "                      'FFT': {'sampling_freq': 128},\n",
    "                      'TargetWrite': 2}#4 -- begin of the window, 2 -- middle of the window, 1 -- end of the window\n",
    "    features_num = X_windowed.shape[2]\n",
    "    if parameters['FilterBank'] is not None:\n",
    "        features_num *= len(parameters['FilterBank']['filters'])\n",
    "    multiplier = 0\n",
    "    if parameters['StackedRecurrence'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedDET'] is not None:\n",
    "        multiplier += 2\n",
    "    if parameters['StackedPoincareSD'] is not None:\n",
    "        multiplier += 2\n",
    "    if parameters['StackedMeanAutocorrelation'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedMeanPeriod'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedAproximateEntropy'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedSampleEntropy'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedSpectralEntropy'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedPFD'] is not None:\n",
    "        multiplier += 1\n",
    "    if parameters['StackedHjorth'] is not None:\n",
    "        multiplier += 3\n",
    "    features_num *= multiplier\n",
    "    if parameters['FFT'] is not None:\n",
    "        features_num += X_windowed.shape[2] * X_windowed.shape[1]\n",
    "    if features_num == 0:\n",
    "        raise BaseException('specify features')\n",
    "    X_transformed = np.zeros((X_windowed.shape[0], features_num))\n",
    "    Y_transformed = np.zeros((Y_windowed.shape[0], Y_windowed.shape[2])).astype(np.int)\n",
    "    X_banded = None\n",
    "    if parameters['FilterBank'] is not None:\n",
    "        X_banded = np.zeros((X_windowed.shape[0], X_windowed.shape[1],\n",
    "                             X_windowed.shape[2] * len(parameters['FilterBank']['filters'])))\n",
    "        for i in range(X_windowed.shape[0]):\n",
    "            X_banded[i] = FilterBank(X=X_windowed[i], filters=parameters['FilterBank']['filters'],\n",
    "                                     sample_rate=parameters['FilterBank']['sample_rate'],\n",
    "                                     butter_pows=parameters['FilterBank']['butter_pows'])\n",
    "    else:\n",
    "        X_banded = X_windowed\n",
    "    print('    banded dataset')\n",
    "    for i in range(X_banded.shape[0]):\n",
    "        step = 0\n",
    "        if parameters['StackedRecurrence'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedRecurrence(X=X_banded[i],\n",
    "                                                               epsilon=parameters['StackedRecurrence']['epsilon'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedDET'] is not None:\n",
    "            X_transformed[i, step: step + 2 * X_banded.shape[2]] = StackedDET(X=X_banded[i],\n",
    "                                                                   epsilon=parameters['StackedDET']['epsilon'],\n",
    "                                                                   min_l=parameters['StackedDET']['min_l'])\n",
    "            step += 2 * X_banded.shape[2]\n",
    "        if parameters['StackedPoincareSD'] is not None:\n",
    "            X_transformed[i, step: step + 2 * X_banded.shape[2]] = StackedPoincareSD(X=X_banded[i])\n",
    "            step += 2 * X_banded.shape[2]\n",
    "        if parameters['StackedMeanAutocorrelation'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedMeanAutocorrelation(X=X_banded[i],\n",
    "                                                        max_lag=parameters['StackedMeanAutocorrelation']['max_lag'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedMeanPeriod'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedMeanPeriod(X=X_banded[i],\n",
    "                                                               max_lag=parameters['StackedMeanPeriod']['max_lag'],\n",
    "                                                             threshold=parameters['StackedMeanPeriod']['threshold'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedAproximateEntropy'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedAproximateEntropy(X=X_banded[i],\n",
    "                                                               M=parameters['StackedAproximateEntropy']['M'],\n",
    "                                                               R=parameters['StackedAproximateEntropy']['R'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedSampleEntropy'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedSampleEntropy(X=X_banded[i],\n",
    "                                                               M=parameters['StackedSampleEntropy']['M'],\n",
    "                                                               R=parameters['StackedSampleEntropy']['R'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedSpectralEntropy'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedSpectralEntropy(X=X_banded[i],\n",
    "                                                sampling_freq=parameters['StackedSpectralEntropy']['sampling_freq'])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedPFD'] is not None:\n",
    "            X_transformed[i, step: step + X_banded.shape[2]] = StackedPFD(X=X_banded[i])\n",
    "            step += X_banded.shape[2]\n",
    "        if parameters['StackedHjorth'] is not None:\n",
    "            X_transformed[i, step: step + 3 * X_banded.shape[2]] = StackedHjorth(X=X_banded[i])\n",
    "            step += 3 * X_banded.shape[2]\n",
    "        if parameters['FFT'] is not None:\n",
    "            X_transformed[i, step: step + X_windowed.shape[2] * X_windowed.shape[1]] = fft.rfft(\n",
    "                                                                                    x=X_windowed[i], axis=0).ravel()\n",
    "            step += X_windowed.shape[2] * X_windowed.shape[1]\n",
    "        print('    step %i completed'%(i))\n",
    "    print('    transformed design matrix')\n",
    "    for i in range(Y_windowed.shape[0]):\n",
    "        if parameters['TargetWrite'] == 4:\n",
    "            Y_transformed[i] = Y_windowed[i, 0, :]\n",
    "        elif parameters['TargetWrite'] == 1:\n",
    "            Y_transformed[i] = Y_windowed[i, -1, :]\n",
    "        else:\n",
    "            Y_transformed[i] = Y_windowed[i, Y_windowed.shape[1] // 2, :]\n",
    "    print('    transformed target matrix')\n",
    "    return X_transformed, Y_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-19T15:12:06.759546",
     "start_time": "2017-04-19T11:53:07.615510"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset\n",
      "windowed dataset\n",
      "completed train/test split\n",
      "completed scaling dataset\n",
      "    banded dataset\n",
      "    transformed design matrix\n",
      "    transformed target matrix\n",
      "    banded dataset\n",
      "    transformed design matrix\n",
      "    transformed target matrix\n",
      "transformed dataset\n",
      "saved transformed dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_np = pd.read_csv('./eeg_eye_state.csv', sep=',').as_matrix()\n",
    "print('loaded dataset')\n",
    "windowed_design_matrix = WindowDataset(dataset_np[:, :14], interval=77, shift=1)\n",
    "windowed_target_matrix = WindowDataset(dataset_np[:, 14][:, np.newaxis], interval=77, shift=1)\n",
    "print('windowed dataset')\n",
    "train_design_windowed, test_design_windowed = windowed_design_matrix[:7452], windowed_design_matrix[7452:]\n",
    "train_target_windowed, test_target_windowed = windowed_target_matrix[:7452], windowed_target_matrix[7452:]\n",
    "print('completed train/test split')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset_np[:7528, :14])\n",
    "for i in range(train_design_windowed.shape[0]):\n",
    "    train_design_windowed[i, :, :] = scaler.transform(train_design_windowed[i, :, :])\n",
    "for i in range(test_design_windowed.shape[0]):\n",
    "    test_design_windowed[i, :, :] = scaler.transform(test_design_windowed[i, :, :])\n",
    "print('completed scaling dataset')\n",
    "train_design_windowed, train_target_windowed = TransformWindowedDataset(train_design_windowed,\n",
    "                                                                        train_target_windowed)\n",
    "test_design_windowed, test_target_windowed = TransformWindowedDataset(test_design_windowed, test_target_windowed)\n",
    "print('transformed dataset')\n",
    "file = open('EEG_eye_state_train_X.pkl', 'wb')\n",
    "pkl.dump(train_design_windowed, file)\n",
    "file.close()\n",
    "file = open('EEG_eye_state_train_Y.pkl', 'wb')\n",
    "pkl.dump(train_target_windowed, file)\n",
    "file.close()\n",
    "file = open('EEG_eye_state_test_X.pkl', 'wb')\n",
    "pkl.dump(test_design_windowed, file)\n",
    "file.close()\n",
    "file = open('EEG_eye_state_test_Y.pkl', 'wb')\n",
    "pkl.dump(test_target_windowed, file)\n",
    "file.close()\n",
    "print('saved transformed dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:27:45.869685",
     "start_time": "2017-04-20T22:27:45.654067"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7452, 2254), (7452, 2254), (7452,), (7452,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('EEG_eye_state_train_X.pkl', 'rb')\n",
    "train_X = pkl.load(file)\n",
    "file.close()\n",
    "file = open('EEG_eye_state_train_Y.pkl', 'rb')\n",
    "train_Y = pkl.load(file).ravel()\n",
    "file.close()\n",
    "file = open('EEG_eye_state_test_X.pkl', 'rb')\n",
    "test_X = pkl.load(file)\n",
    "file.close()\n",
    "file = open('EEG_eye_state_test_Y.pkl', 'rb')\n",
    "test_Y = pkl.load(file).ravel()\n",
    "file.close()\n",
    "train_X.shape, test_X.shape, train_Y.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:28:10.438446",
     "start_time": "2017-04-20T22:27:46.464341"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7452, 532), (7452, 532), (7452,), (7452,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X, test_X = scaler.transform(train_X), scaler.transform(test_X)\n",
    "transformer = PCA(n_components=0.95, svd_solver='full')\n",
    "transformer.fit(train_X)\n",
    "train_X, test_X = transformer.transform(train_X), transformer.transform(test_X)\n",
    "scaler.fit(train_X)\n",
    "train_X, test_X = scaler.transform(train_X), scaler.transform(test_X)\n",
    "train_X.shape, test_X.shape, train_Y.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:31:23.408090",
     "start_time": "2017-04-20T22:28:12.510388"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine ROC AUC score: 0.637569753399\n",
      "Support Vector Machine accuracy score: 0.587761674718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Support Vector Machine ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Support Vector Machine accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:31:25.120058",
     "start_time": "2017-04-20T22:31:23.409995"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ROC AUC score: 0.591191904632\n",
      "Logistic Regression accuracy score: 0.550858829844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "clf = LR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Logistic Regression ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Logistic Regression accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:32:54.937033",
     "start_time": "2017-04-20T22:31:25.129499"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors ROC AUC score: 0.569893762905\n",
      "K Nearest Neighbors accuracy score: 0.555018786903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "\n",
    "\n",
    "clf = KNC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('K Nearest Neighbors ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('K Nearest Neighbors accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:00.436524",
     "start_time": "2017-04-20T22:32:54.938502"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree ROC AUC score: 0.556414196112\n",
      "Decision Tree accuracy score: 0.540794417606\n"
     ]
    }
   ],
   "source": [
    "clf = DTC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Decision Tree ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Decision Tree accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:01.875240",
     "start_time": "2017-04-20T22:33:00.439034"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ROC AUC score: 0.533742919475\n",
      "Random Forest accuracy score: 0.541196994096\n"
     ]
    }
   ],
   "source": [
    "clf = RFC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Random Forest ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Random Forest accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:02.330504",
     "start_time": "2017-04-20T22:33:01.877347"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees ROC AUC score: 0.54358779072\n",
      "Extra Trees ROC AUC score: 0.539586688137\n"
     ]
    }
   ],
   "source": [
    "clf = ETC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Extra Trees ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Extra Trees ROC AUC score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:25.416181",
     "start_time": "2017-04-20T22:33:02.331903"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost ROC AUC score: 0.563101328451\n",
      "AdaBoost ROC AUC score: 0.533413848631\n"
     ]
    }
   ],
   "source": [
    "clf = ABC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('AdaBoost ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('AdaBoost ROC AUC score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:35.367404",
     "start_time": "2017-04-20T22:33:25.417665"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme Gradient Boosting ROC AUC score: 0.598028361058\n",
      "Extreme Gradient Boosting accuracy score: 0.564143853999\n"
     ]
    }
   ],
   "source": [
    "clf = XGBC()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Extreme Gradient Boosting ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Extreme Gradient Boosting accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:36.272392",
     "start_time": "2017-04-20T22:33:35.369716"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis ROC AUC score: 0.595051423897\n",
      "Linear Discriminant Analysis accuracy score: 0.554079441761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/discriminant_analysis.py:523: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "clf = LDA()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Linear Discriminant Analysis ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Linear Discriminant Analysis accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:33:37.632542",
     "start_time": "2017-04-20T22:33:36.276130"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic Discriminant Analysis ROC AUC score: 0.511011453504\n",
      "Quadratic Discriminant Analysis accuracy score: 0.386205045625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "\n",
    "clf = QDA()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict_proba(test_X)[:, 1]\n",
    "print('Quadratic Discriminant Analysis ROC AUC score:', roc_auc_score(test_Y, preds))\n",
    "preds = clf.predict(test_X)\n",
    "print('Quadratic Discriminant Analysis accuracy score:', accuracy_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:34:24.771179",
     "start_time": "2017-04-20T22:33:37.636201"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine ROC AUC score: 0.637829833651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "clf = SVR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('Support Vector Machine ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:35:08.039094",
     "start_time": "2017-04-20T22:34:24.773093"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors ROC AUC score: 0.569893762905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "\n",
    "\n",
    "clf = KNR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('K Nearest Neighbors ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:35:12.538922",
     "start_time": "2017-04-20T22:35:08.040465"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree ROC AUC score: 0.555421247419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "\n",
    "\n",
    "clf = DTR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('Decision Tree ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:35:47.192874",
     "start_time": "2017-04-20T22:35:12.540880"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ROC AUC score: 0.586031555573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "\n",
    "\n",
    "clf = RFR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('Random Forest ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:35:52.549165",
     "start_time": "2017-04-20T22:35:47.194926"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees ROC AUC score: 0.594322256418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor as ETR\n",
    "\n",
    "\n",
    "clf = ETR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('Extra Trees ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:36:04.894786",
     "start_time": "2017-04-20T22:35:52.550396"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost ROC AUC score: 0.582880166738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor as ABR\n",
    "\n",
    "\n",
    "clf = ABR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('AdaBoost ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:36:16.535183",
     "start_time": "2017-04-20T22:36:04.897478"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme Gradient Boosting ROC AUC score: 0.594125248354\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBRegressor as XGBR\n",
    "\n",
    "\n",
    "clf = XGBR()\n",
    "clf.fit(train_X, train_Y)\n",
    "preds = clf.predict(test_X)\n",
    "print('Extreme Gradient Boosting ROC AUC score:', roc_auc_score(test_Y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:36:16.574064",
     "start_time": "2017-04-20T22:36:16.537401"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_np = pd.read_csv('./eeg_eye_state.csv', sep=',').as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:36:16.902723",
     "start_time": "2017-04-20T22:36:16.576635"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from mne.decoding import CSP\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def CSPWindowDataset(dataset, interval=77, shift=1):\n",
    "    if (interval < 1) or (interval > dataset.shape[0]):\n",
    "        raise BaseException('interval has invalid value')\n",
    "    if (shift < 1) or (shift > interval):\n",
    "        raise BaseException('shift has invalid value')\n",
    "    windowed_dataset = np.zeros((np.ceil((dataset.shape[0] - interval + 1) / shift).astype(np.int),\n",
    "                                 dataset.shape[1], interval))\n",
    "    begin = 0\n",
    "    for i in range(windowed_dataset.shape[0]):\n",
    "        windowed_dataset[i] = dataset[begin: begin + interval, :].copy().T\n",
    "        begin += shift\n",
    "    return windowed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:36:17.021553",
     "start_time": "2017-04-20T22:36:16.904198"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b, a = sig.butter(5, 2 * np.array([1.0, 35]) / 128, btype='bandpass')\n",
    "dataset_np[:, :14] = sig.lfilter(b, a, 1e-6*dataset_np[:, :14], axis=0)\n",
    "windowed_dataset = CSPWindowDataset(dataset_np[:, :14])\n",
    "windowed_target = WindowDataset(dataset_np[:, 14][:, np.newaxis])\n",
    "target = np.zeros(windowed_target.shape[0]).astype(np.int)\n",
    "for i in range(target.shape[0]):\n",
    "    target[i] = windowed_target[i, windowed_target.shape[1] // 2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T22:37:53.485706",
     "start_time": "2017-04-20T22:36:17.023036"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cross validation completed: Support Vector Machine algorithm\n",
      "    mean roc auc is: 0.813186011431\n",
      "Pipeline cross validation completed: Logistic Regression algorithm\n",
      "    mean roc auc is: 0.546761021336\n",
      "Pipeline cross validation completed: k Nearest Neighbors algorithm\n",
      "    mean roc auc is: 0.990047623206\n",
      "Pipeline cross validation completed: Decision Tree algorithm\n",
      "    mean roc auc is: 0.923263249909\n",
      "Pipeline cross validation completed: Random Forest algorithm\n",
      "    mean roc auc is: 0.986248318377\n",
      "Pipeline cross validation completed: Extra Trees algorithm\n",
      "    mean roc auc is: 0.992163066001\n",
      "Pipeline cross validation completed: Ada Boost algorithm\n",
      "    mean roc auc is: 0.782153174349\n",
      "Pipeline cross validation completed: Extreme Gradient Boosting algorithm\n",
      "    mean roc auc is: 0.865977248196\n",
      "Pipeline cross validation completed: Linear Discriminant Analysis algorithm\n",
      "    mean roc auc is: 0.542754069951\n",
      "Pipeline cross validation completed: Quadratic Discriminant Analysis algorithm\n",
      "    mean roc auc is: 0.649176556817\n",
      "Pipeline cross validation completed: Support Vector Machine (Regressor) algorithm\n",
      "    mean roc auc is: 0.819572461962\n",
      "Pipeline cross validation completed: k Nearest Neighbors (Regressor) algorithm\n",
      "    mean roc auc is: 0.990047623206\n",
      "Pipeline cross validation completed: Decision Tree (Regressor) algorithm\n",
      "    mean roc auc is: 0.925422920218\n",
      "Pipeline cross validation completed: Random Forest (Regressor) algorithm\n",
      "    mean roc auc is: 0.984546729839\n",
      "Pipeline cross validation completed: Extra Trees (Regressor) algorithm\n",
      "    mean roc auc is: 0.993324262304\n",
      "Pipeline cross validation completed: Ada Boost (Regressor) algorithm\n",
      "    mean roc auc is: 0.669020905527\n",
      "Pipeline cross validation completed: Extreme Gradient Boosting (Regressor) algorithm\n",
      "    mean roc auc is: 0.874211278696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "csp = CSP(reg='ledoit_wolf')\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=317)\n",
    "algorithms = [(SVC(probability=True), 'Support Vector Machine'),\n",
    "              (LR(), 'Logistic Regression'),\n",
    "              (KNC(), 'k Nearest Neighbors'),\n",
    "              (DTC(), 'Decision Tree'),\n",
    "              (RFC(), 'Random Forest'),\n",
    "              (ETC(), 'Extra Trees'),\n",
    "              (ABC(), 'Ada Boost'),\n",
    "              (XGBC(), 'Extreme Gradient Boosting'),\n",
    "              (LDA(), 'Linear Discriminant Analysis'),\n",
    "              (QDA(), 'Quadratic Discriminant Analysis'),\n",
    "              (SVR(), 'Support Vector Machine (Regressor)'),\n",
    "              (KNR(), 'k Nearest Neighbors (Regressor)'),\n",
    "              (DTR(), 'Decision Tree (Regressor)'),\n",
    "              (RFR(), 'Random Forest (Regressor)'),\n",
    "              (ETR(), 'Extra Trees (Regressor)'),\n",
    "              (ABR(), 'Ada Boost (Regressor)'),\n",
    "              (XGBR(), 'Extreme Gradient Boosting (Regressor)')]\n",
    "for alg, name in algorithms:\n",
    "    clf = Pipeline([('CSP', csp), (name, alg)])\n",
    "    scores = cross_val_score(clf, windowed_dataset, target, scoring='roc_auc', cv=cv.split(target), n_jobs=1)\n",
    "    print('Pipeline cross validation completed:', name, 'algorithm')\n",
    "    print('    mean roc auc is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
